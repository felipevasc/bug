#!/usr/bin/env python3
"""
@skill: python/exploit/ssrf-check
@inputs: target[, url, out-dir, scope-file, rate, timeout, allow-exploit]
@outputs: finding|note
@tools: curl, nuclei
"""

import argparse
import json
import os
import re
import shutil
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path
from urllib.parse import urlsplit, urlunsplit


def now_iso():
    return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")


def emit(record):
    if "ts" not in record:
        record["ts"] = now_iso()
    if "timestamp" not in record:
        record["timestamp"] = record["ts"]
    sys.stdout.write(json.dumps(record, separators=(",", ":")) + "\n")


def load_scope(scope_file):
    if not scope_file:
        return []
    p = Path(scope_file)
    if not p.exists():
        return []
    entries = []
    for line in p.read_text(encoding="utf-8", errors="ignore").splitlines():
        s = line.strip()
        if not s or s.startswith("#"):
            continue
        entries.append(s)
    return entries


def hostname_in_scope(host, entry):
    h = host.lower().rstrip(".")
    e = entry.lower().rstrip(".")
    if e.startswith("*."):
        root = e[2:]
        return h == root or h.endswith("." + root)
    return h == e or h.endswith("." + e)


def is_ipv4(s: str) -> bool:
    return re.match(r"^\\d+\\.\\d+\\.\\d+\\.\\d+$", s) is not None


def is_ipv4_cidr(s: str) -> bool:
    return re.match(r"^\\d+\\.\\d+\\.\\d+\\.\\d+/\\d{1,2}$", s) is not None


def normalize_url(raw: str) -> str:
    s = (raw or "").strip()
    if not s:
        return ""
    if not re.match(r"^[a-zA-Z][a-zA-Z0-9+.-]*://", s):
        s = "https://" + s
    try:
        sp = urlsplit(s)
        sp = sp._replace(fragment="")
        return urlunsplit(sp)
    except Exception:
        return s


def url_host_path(raw: str):
    try:
        sp = urlsplit(raw)
        host = (sp.hostname or "").lower().rstrip(".")
        path = sp.path or "/"
        if not path.startswith("/"):
            path = "/" + path
        return host, path
    except Exception:
        return "", "/"


def normalize_path_prefix(p: str) -> str:
    if not p or p == "/":
        return "/"
    out = p if p.startswith("/") else "/" + p
    if len(out) > 1 and out.endswith("/"):
        out = out[:-1]
    return out


def path_matches_prefix(target_path: str, entry_path: str) -> bool:
    prefix = normalize_path_prefix(entry_path)
    if prefix == "/":
        return True
    target_norm = normalize_path_prefix(target_path)
    if target_norm == prefix:
        return True
    return target_norm.startswith(prefix + "/")


def target_in_scope(target, entries):
    if not entries:
        return True
    t = (target or "").strip()
    if not t:
        return False

    t_is_url = "://" in t or "/" in t or "?" in t or "#" in t
    t_url = normalize_url(t) if t_is_url else ""
    t_host, t_path = url_host_path(t_url) if t_is_url else (t.lower().rstrip("."), "")

    for e in entries:
        e = (e or "").strip()
        if not e:
            continue
        if is_ipv4(e) or is_ipv4_cidr(e):
            continue
        if "://" in e or "/" in e:
            e_url = normalize_url(e)
            e_host, e_path = url_host_path(e_url)
            if not e_host:
                continue
            if not hostname_in_scope(t_host, e_host):
                continue
            if e_path and e_path != "/":
                if not t_is_url:
                    continue
                if not path_matches_prefix(t_path, e_path):
                    continue
            return True
        if hostname_in_scope(t_host, e):
            return True
    return False


def run_capture(cmd, timeout_s):
    try:
        res = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout_s,
            check=False,
        )
        return res.returncode, res.stdout, res.stderr
    except subprocess.TimeoutExpired:
        return 124, "", "timeout"


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--target", required=True)
    parser.add_argument("--url", default=None)
    parser.add_argument("--out-dir", default=None)
    parser.add_argument("--scope-file", default=None)
    parser.add_argument("--rate", default=None)
    parser.add_argument("--timeout", default=None)
    parser.add_argument("--allow-exploit", action="store_true")
    args = parser.parse_args()

    source = "src/skills/python/exploit/01-ssrf-check.py"
    stage = "exploit"
    target = args.target
    raw_url = args.url or os.environ.get("TARGET_URL") or ""
    target_url = normalize_url(raw_url) if raw_url else ""
    scope_target = target_url or target
    scope_entries = load_scope(args.scope_file)
    if not target_in_scope(scope_target, scope_entries):
        emit({
            "type": "note",
            "tool": "scope",
            "stage": stage,
            "target": target,
            "severity": "info",
            "evidence": [f"out_of_scope: {scope_target}"],
            "data": {"reason": "target not in scope (blocked)"},
            "source": source,
        })
        return

    timeout_s = int(float(args.timeout or 20))
    run_ts = os.environ.get("RUN_TS") or "run"
    root_out = Path(args.out_dir or os.environ.get("OUT_DIR") or Path("data") / "runs" / run_ts)
    ev_dir = root_out / "evidence" / "exploit" / "ssrf"
    ev_dir.mkdir(parents=True, exist_ok=True)

    # Safe-by-default heuristic: fetch homepage and look for common SSRF sink parameter names in links.
    if not shutil.which("curl"):
        emit({
            "type": "note",
            "tool": "curl",
            "stage": stage,
            "target": target,
            "severity": "info",
            "evidence": [],
            "data": {"skipped": True, "reason": "tool not found"},
            "source": source,
        })
        return

    url = target_url or f"https://{target}/"
    code, out, err = run_capture(["curl", "-k", "-sS", "--max-time", str(timeout_s), url], timeout_s + 5)
    ev_html = ev_dir / f"{target}.homepage.html"
    ev_html.write_text(out, encoding="utf-8", errors="ignore")
    evidence = [str(ev_html)]

    sinks = []
    # Very naive: match href with query string and extract parameter names.
    for m in re.finditer(r"href=[\"']([^\"']+\\?[^\"']+)[\"']", out, flags=re.IGNORECASE):
        href = m.group(1)
        if "?" not in href:
            continue
        qs = href.split("?", 1)[1]
        for kv in qs.split("&"):
            k = kv.split("=", 1)[0].strip().lower()
            if k in ["url", "uri", "path", "dest", "destination", "redirect", "next", "continue", "return", "callback", "site", "image", "img", "file", "feed", "data"]:
                sinks.append({"href": href[:300], "param": k})

    sinks = sinks[:50]
    if sinks:
        ev_json = ev_dir / f"{target}.ssrf-sinks.json"
        ev_json.write_text(json.dumps(sinks, indent=2), encoding="utf-8", errors="ignore")
        evidence.append(str(ev_json))
        emit({
            "type": "finding",
            "tool": "ssrf-heuristic",
            "stage": stage,
            "target": target,
            "severity": "low",
            "evidence": evidence,
            "data": {"potential_sinks": sinks},
            "source": source,
        })
    else:
        emit({
            "type": "note",
            "tool": "ssrf-heuristic",
            "stage": stage,
            "target": target,
            "severity": "info",
            "evidence": evidence,
            "data": {"message": "no obvious SSRF sink parameters found on homepage"},
            "source": source,
        })

    # Gated: nuclei SSRF templates (still non-destructive but intrusive scanning).
    if not (args.allow_exploit and os.environ.get("CONFIRM") == "arrocha!"):
        emit({
            "type": "note",
            "tool": "exploit-gate",
            "stage": stage,
            "target": target,
            "severity": "info",
            "evidence": [],
            "data": {"intrusive_actions": "blocked", "required": "--allow-exploit + env CONFIRM=arrocha!"},
            "source": source,
        })
        return

    if not shutil.which("nuclei"):
        emit({
            "type": "note",
            "tool": "nuclei",
            "stage": stage,
            "target": target,
            "severity": "info",
            "evidence": [],
            "data": {"skipped": True, "reason": "tool not found"},
            "source": source,
        })
        return

    out_jsonl = ev_dir / f"{target}.nuclei-ssrf.jsonl"
    nuclei_target = target_url or target
    cmd = ["nuclei", "-target", nuclei_target, "-jsonl", "-silent", "-tags", "ssrf", "-o", str(out_jsonl)]
    code, out, err = run_capture(cmd, max(timeout_s, 60))
    # nuclei writes to file; keep path as evidence
    emit({
        "type": "note",
        "tool": "nuclei",
        "stage": stage,
        "target": target,
        "severity": "info",
        "evidence": [str(out_jsonl)],
        "data": {"message": "nuclei SSRF tag run completed (review evidence)"},
        "source": source,
    })


if __name__ == "__main__":
    main()
